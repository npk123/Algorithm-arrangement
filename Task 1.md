1.机器学习的概念

-1.1 有监督

   监督学习(supervised Learning)的数据集中的每个样本都有相应的“正确答案”，再根据这些样本作出预测，训练资料是由
输入物件（通常是向量）和预期输出所组成。函数的输出可以是一个连续的值（称为回归分析），或是预测一个分类标签（称作分类）。
 
-1.2 无监督

   无监督式学习(Unsupervised Learning )目的是对原始资料进行分类，以便了解资料内部结构。
   
   有别于监督式学习网络，无监督式学习网络在学习时并不知道其分类结果是否正确，亦即没有受到监督式增强(告诉它何种学习是正确的)。
其特点是仅对此种网络提供输入范例，而它会自动从这些范例中找出其潜在类别规则。典型例子就是聚类（Clustering）。

-1.3 泛化能力

   泛化能力（generalization ability）是指机器学习算法对新鲜样本的识别能力和对未知数据的预测能力。
   
   一般来讲，模型或算法的学习能力低下是因为其对数据深层次的规律特点并未全面掌握，而只是“死记硬背”的将样本数据
特征进行了记录，这就是过拟合导致的。
 
-1.4 过拟合，欠拟合（方差和偏差以及各自解决办法）

   对于训练好的模型，若在训练集表现差，不必说在测试集表现同样会很差，这可能是欠拟合导致；若模型在训练集表现非常好，
却在测试集上差强人意，则这便是过拟合导致的。
 
   欠拟合会导致高 Bias ，过拟合会导致高 Variance ，所以模型需要在 Bias 与 Variance 之间做出一个权衡。
   
   使用简单的模型去拟合复杂数据时，会导致模型很难拟合数据的真实分布，这时模型便欠拟合了，或者说有很大的 Bias，
Bias 即为模型的期望输出与其真实输出之间的差异；有时为了得到比较精确的模型而过度拟合训练数据，或者模型复杂度
过高时，可能连训练数据的噪音也拟合了，导致模型在训练集上效果非常好，但泛化性能却很差，这时模型便过拟合了，
或者说有很大的 Variance，这时模型在不同训练集上得到的模型波动比较大，Variance 刻画了不同训练集得到的模型的
输出与这些模型期望输出的差异。
 
--1.4.1 解决欠拟合的方法：

    1、增加新特征，可以考虑加入其他特征组合、高次特征，来增大假设空间;
    2、尝试非线性模型，比如核SVM 、决策树、DNN等模型;
    3、减少正则化参数，正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数;
    4、Boosting ,Boosting 往往会有较小的 Bias，比如 Gradient Boosting 等。
    
--1.4.2 解决过拟合的方法：

    1、交叉检验，通过交叉检验得到较优的模型参数;
    2、早停策略。本质上是交叉验证策略，选择合适的训练次数，避免训练的网络过度拟合训练数据。
    3、特征选择，减少特征数或使用较少的特征组合，对于按区间离散化的特征，增大划分的区间;
    4、正则化，常用的有 L1、L2 正则。而且 L1正则还可以自动进行特征选择;
    5、如果有正则项则可以考虑增大正则项参数;
    6、增加训练数据可以有效的避免过拟合。

-1.5 交叉验证

   交叉验证是一种模型验证技术，可用于评估统计分析（模型）结果在其它独立数据集上的泛化能力。其目标是定义一
个数据集，以便于在训练阶段（例如，验证数据集）中测试模型，从而限制模型过拟合、欠拟合等问题，并且帮助我们了
解模型在其它独立数据集上的泛化能力。值得一提的是，验证集和训练集必须满足独立同分布条件，否则交叉验证只会让结果变得更加糟糕。
 
   第一种是简单交叉验证，所谓的简单，是和其他交叉验证方法相对而言的。首先，我们随机的将样本数据分为两部分
（比如： 70%的训练集，30%的测试集），然后用训练集来训练模型，在测试集上验证模型及参数。接着，我们再把
样本打乱，重新选择训练集和测试集，继续训练数据和检验模型。最后我们选择损失函数评估最优的模型和参数。　
 （Python 的实现方式：sklearn.model_selection.train_test_split）
 
   第二种是K折交叉验证（K-Folder Cross Validation）。和第一种方法不同，K折交叉验证会把样本数据随机的分
成S份，每次随机的选择K-1份作为训练集，剩下的1份做测试集。当这一轮完成后，重新随机选择K-1份来训练数据。
若干轮（小于K）之后，选择损失函数评估最优的模型和参数。通常情况下，我们设置 k=5 或 k=10。
 （Python 实现代码：sklearn.model_selection.KFold）

   第三种是留一交叉验证（Leave-one-out Cross Validation），它是第二种情况的特例，此时K等于样本数N，这
样对于N个样本，每次选择N-1个样本来训练数据，留1个样本来验证模型预测的好坏。此方法主要用于样本量非常少
的情况，比如对于普通适中问题，N小于50时一般采用留一交叉验证。
 （Python 实现代码：sklearn.model_selection.LeaveOneOut）

2.线性回归原理

   尝试使用一条直线来拟合数据，使所有点到直线的距离之和最小。实际上，线性回归中通常使用残差平方和，即点到
直线的平行于y轴的距离而不用垂线距离，残差平方和除以样本量n就是均方误差。均方误差作为线性回归模型的代价函数(cost function)。
使所有点到直线的距离之和最小，就是使均方误差最小化，这个方法叫做线性回归法。
   
3.各类函数

   损失函数和代价函数是同一个东西，目标函数(Objective function)是一个与他们相关但更广的概念，对于目标函数来
说在有约束条件下的最小化就是损失函数（loss function）。

-3.1 损失函数(loss function)与代价函数(cost function)

   线性回归模型的代价函数(cost function)是用来度量拟合的程度，其本质为均方误差和。即使所有点到直线的距离之和最小，就是使均方误差最小化，
损失函数越小，就代表模型拟合的越好。

-3.2 目标函数(Objective function)

   即将代价函数加上正则化的束缚因而兼顾模型结构的简单度和均方误差和的最小化。

4.优化方法

-4.1 梯度下降

   损失函数就是一个自变量为算法的参数，函数值为误差值的函数。而梯度下降就是找让误差值最小时候算法取的参数。
   因此梯度就是导数。梯度下降作用是找到函数的最小值所对应的自变量的值（曲线最低点对应x的值）。目的是为了找x。
   梯度下降含义（具体操作）是：改变x的值使得导数的绝对值变小，当导数小于0时候，我们要让目前x值大一点点，再看它导数值。
当导数大于0时候，我们要让目前x值减小一点点，再看它导数值。当导数接近0时候，我们就得到想要的自变量x了。也就是说找到这个算法最佳参数，
使得拟合曲线与真实值误差最小。

-4.2 牛顿法

   牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f (x)的泰勒级数的前面几项来寻找方程f (x) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。
   基本思想是：在现有极小值估计值的附近对f(x)作二阶泰勒展开，进而找极小点的下一个估计值。设xk为当前的极小点估计值，则
   ![equation1](https://github.com/npk123/Algorithm-arrangement/blob/master/images/222309088311820.png)
   由于求得是最值，由极值必要条件可知，f(x)应该满足f′(x)=0，即
   ![equation1](https://github.com/npk123/Algorithm-arrangement/blob/master/images/222309221284615.png)
   由于牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是"切线法"。
   从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。如果更通俗地说的话，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。（牛顿法目光更加长远，所以少走弯路；相对而言，梯度下降法只考虑了局部的最优，没有全局思想。）
   牛顿法的优缺点总结：
   优点：二阶收敛，收敛速度快；
   缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。
   
   Ref：https://www.cnblogs.com/shixiangwan/p/7532830.html

-4.3 拟牛顿法

5.评估指标

6.sklearn参数详解
