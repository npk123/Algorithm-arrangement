# 1、逻辑回归与线性回归的联系与区别

联系：

线性回归：根据几组已知数据和拟合函数训练其中未知参数，使得拟合损失达到最小。然后用所得的拟合函数进行预测。 

逻辑回归：和拟合函数训练其中未知参数使得对数似然函数最大。然后用所得的拟合函数进行二分类。 

两者都是回归，步骤和原理看起来相似。但线性回归的应用场合大多是回归分析，一般不用在分类问题上。原因可以概括为以下两个：

    1.回归模型是连续型模型，即预测出的值都是连续值（实数值），非离散值；
    2.预测结果受样本噪声的影响比较大。

区别如下图所示
  ![equation1](https://github.com/npk123/Algorithm-datawhale/blob/master/images/Capture.JPG)

1. 拟合函数和预测函数的关系，其实就是将拟合函数做了一个逻辑函数的转换。 

2. 最小二乘和最大似然估计不可以相互替代。原理：最大似然估计是计算使得数据出现的可能性最大的参数，依仗的自然
是Probability。而最小二乘是计算误差损失。

区别：

    1.线性回归要求变量服从正态分布，logistic回归对变量分布没有要求。
    2.线性回归要求因变量是连续性数值变量，而logistic回归要求因变量是分类型变量。
    3.线性回归要求自变量和因变量呈线性关系，而logistic回归不要求自变量和因变量呈线性关系
    4.logistic回归是分析因变量取某个值的概率与自变量的关系，而线性回归是直接分析因变量与自变量的关系

总之, logistic回归与线性回归实际上有很多相同之处，最大的区别就在于他们的因变量不同，其他的基本都差不多，
正是因为如此，这两种回归可以归于同一个家族，即广义线性模型（generalized linear model）。这一家族中的模
型形式基本上都差不多，不同的就是因变量不同，如果是连续的，就是多重线性回归，如果是二项分布，就是logistic回归。
logistic回归的因变量可以是二分类的，也可以是多分类的，但是二分类的更为常用，也更加容易解释。所以实际中最为常用的就是二分类的logistic回归。

# 2、 逻辑回归的原理

逻辑回归是应用非常广泛的一个分类机器学习算法，它将数据拟合到一个logit函数(或者叫做logistic函数)中，从而能够完成对事件发生的概率进行预测。

它的核心思想是，如果线性回归的结果输出是一个连续值，而值的范围是无法限定的，那我们有没有办法把这个结果值映射为可以帮助我们判断的结果呢。而如果输出结果是 (0,1) 的一个概率值，这个问题就很清楚了。数学上使用便是sigmoid函数(如下)：

  ![equation1](https://github.com/npk123/Algorithm-datawhale/blob/master/images/sigmoid%20function.jpg)
  
它的输入范围为−∞→+∞，而值域刚好为(0,1)，正好满足概率分布为(0,1)的要求。用概率去描述分类器，自然要比阈值要来的方便。
而且它是一个单调上升的函数，具有良好的连续性，不存在不连续点。
  
  其求导后为
  
  ![equation1](https://github.com/npk123/Algorithm-datawhale/blob/master/images/sigmoid'.jpg)

# 3、逻辑回归损失函数推导及优化

损失函数

  ![equation1](https://github.com/npk123/Algorithm-datawhale/blob/master/images/logistic-loss%20function.jpg)
  
简化过后的损失函数如下
  
  ![equation1](https://github.com/npk123/Algorithm-datawhale/blob/master/images/simplified%20logistic%20loss%20function.png)
  
从数学上理解，我们为了找到最小值点，就应该朝着下降速度最快的方向(导函数/偏导方向)迈进，每次迈进一小步，再看看此时的下降最快方向是哪，再朝着这个方向迈进，直至最低点。

用迭代公式表示出来的最小化J(θ)的梯度下降算法如下：
  
  ![equation1](https://github.com/npk123/Algorithm-datawhale/blob/master/images/%E4%B8%8B%E8%BD%BD1.png)
  
  ![equation1](https://github.com/npk123/Algorithm-datawhale/blob/master/images/%E4%B8%8B%E8%BD%BD2.png)

4、 正则化与模型评估指标

当模型的参数过多时，很容易遇到过拟合的问题。这时就需要有一种方法来控制模型的复杂度，典型的做法在优化目标中加入正则项，
通过惩罚过大的参数来防止过拟合.



# 5、 逻辑回归的优缺点

优点

    1.LR是以概率的形式输出结果，不只是0和1的判定； 
    2.LR的可解释强，可控性高； 
    3.训练快，feature engineering之后效果赞； 
    4.因为结果是概率，可以做ranking model； 
    5.添加feature简单。 
    
缺点

    1.容易欠拟合，分类精度不高;
    2.数据特征有缺失或者特征空间很大时表现效果并不好。
   
LR的应用场景很多哈： 

    1.CTR预估、推荐系统的learning to rank； 
    2.一些电商搜索排序基线； 
    3.一些电商的购物搭配推荐； 
    4.新闻app排序基线。

# 6、 样本不均衡问题解决办法

样本太大怎么处理？ 

    1.对特征离散化，离散化后用one-hot编码处理成0,1值，再用LR处理会较快收敛； 
    2.如果一定要用连续值的话，可以做scaling； 
    3.工具的话有 spark Mllib，它损失了一小部分的准确度达到速度的提升； 
    4.如果没有并行化平台，想做大数据就试试采样。需要注意采样数据，最好不要随机取，可以按照日期/用户/行为，来分层抽样。 

怎么使样本平衡？ 

    1.如果样本不均衡，样本充足的情况下可以做下采样——抽样，样本不足的情况下做上采样——对样本少的做重复； 
    2.修改损失函数，给不同权重。比如负样本少，就可以给负样本大一点的权重； 
    3.采样后的predict结果，用作判定请还原。

7.  sklearn参数

# 8. LR与SVM
两种方法都是常见的分类算法，从目标函数来看，区别在于逻辑回归采用的是logistical loss，svm采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。两者的根本目的都是一样的。此外，根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。但是逻辑回归相对来说模型更简单，好理解，实现起来，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些。但是SVM的理论基础更加牢固，有一套结构化风险最小化的理论基础，虽然一般使用的人不太会去关注。还有很重要的一点，SVM转化为对偶问题后，分类只需要计算与少数几个支持向量的距离，这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算量。

两者对异常的敏感度也不一样。同样的线性分类情况下，如果异常点较多的话，无法剔除，首先LR，LR中每个样本都是有贡献的，最大似然后会自动压制异常的贡献，SVM+软间隔对异常还是比较敏感，因为其训练只需要支持向量，有效样本本来就不高，一旦被干扰，预测结果难以预料。
