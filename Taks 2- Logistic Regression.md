# 1、逻辑回归与线性回归的联系与区别

联系：

线性回归：根据几组已知数据和拟合函数训练其中未知参数，使得拟合损失达到最小。然后用所得的拟合函数进行预测。 

逻辑回归：和拟合函数训练其中未知参数使得对数似然函数最大。然后用所得的拟合函数进行二分类。 

两者都是回归，步骤和原理看起来相似。但线性回归的应用场合大多是回归分析，一般不用在分类问题上。原因可以概括为以下两个：

    1.回归模型是连续型模型，即预测出的值都是连续值（实数值），非离散值；
    2.预测结果受样本噪声的影响比较大。

区别如下图所示
  ![equation1](https://github.com/npk123/Algorithm-datawhale/blob/master/images/Capture.JPG)

1. 拟合函数和预测函数的关系，其实就是将拟合函数做了一个逻辑函数的转换。 

2. 最小二乘和最大似然估计不可以相互替代。原理：最大似然估计是计算使得数据出现的可能性最大的参数，依仗的自然
是Probability。而最小二乘是计算误差损失。

区别：

    1.线性回归要求变量服从正态分布，logistic回归对变量分布没有要求。
    2.线性回归要求因变量是连续性数值变量，而logistic回归要求因变量是分类型变量。
    3.线性回归要求自变量和因变量呈线性关系，而logistic回归不要求自变量和因变量呈线性关系
    4.logistic回归是分析因变量取某个值的概率与自变量的关系，而线性回归是直接分析因变量与自变量的关系

总之, logistic回归与线性回归实际上有很多相同之处，最大的区别就在于他们的因变量不同，其他的基本都差不多，
正是因为如此，这两种回归可以归于同一个家族，即广义线性模型（generalized linear model）。这一家族中的模
型形式基本上都差不多，不同的就是因变量不同，如果是连续的，就是多重线性回归，如果是二项分布，就是logistic回归。
logistic回归的因变量可以是二分类的，也可以是多分类的，但是二分类的更为常用，也更加容易解释。所以实际中最为常用的就是二分类的logistic回归。

2、 逻辑回归的原理

逻辑回归是应用非常广泛的一个分类机器学习算法，它将数据拟合到一个logit函数(或者叫做logistic函数)中，从而能够完成对事件发生的概率进行预测。

它的核心思想是，如果线性回归的结果输出是一个连续值，而值的范围是无法限定的，那我们有没有办法把这个结果值映射为可以帮助我们判断的结果呢。而如果输出结果是 (0,1) 的一个概率值，这个问题就很清楚了。数学上使用便是sigmoid函数(如下)：

  ![equation1](https://github.com/npk123/Algorithm-datawhale/blob/master/images/sigmoid%20function.jpg)
  
它的输入范围为−∞→+∞，而值域刚好为(0,1)，正好满足概率分布为(0,1)的要求。用概率去描述分类器，自然要比阈值要来的方便。
而且它是一个单调上升的函数，具有良好的连续性，不存在不连续点。
  
  其求导后为
![equation1](https://github.com/npk123/Algorithm-datawhale/blob/master/images/sigmoid'.jpg)

3、逻辑回归损失函数推导及优化



4、 正则化与模型评估指标



5、 逻辑回归的优缺点

优点

    1.LR是以概率的形式输出结果，不只是0和1的判定； 
    2.LR的可解释强，可控性高； 
    3.训练快，feature engineering之后效果赞； 
    4.因为结果是概率，可以做ranking model； 
    5.添加feature简单。 
   
LR的应用场景很多哈： 

    1.CTR预估、推荐系统的learning to rank； 
    2.一些电商搜索排序基线； 
    3.一些电商的购物搭配推荐； 
    4.新闻app排序基线。

6、 样本不均衡问题解决办法

样本太大怎么处理？ 

    1.对特征离散化，离散化后用one-hot编码处理成0,1值，再用LR处理会较快收敛； 
    2.如果一定要用连续值的话，可以做scaling； 
    3.工具的话有 spark Mllib，它损失了一小部分的准确度达到速度的提升； 
    4.如果没有并行化平台，想做大数据就试试采样。需要注意采样数据，最好不要随机取，可以按照日期/用户/行为，来分层抽样。 

怎么使样本平衡？ 

    1.如果样本不均衡，样本充足的情况下可以做下采样——抽样，样本不足的情况下做上采样——对样本少的做重复； 
    2.修改损失函数，给不同权重。比如负样本少，就可以给负样本大一点的权重； 
    3.采样后的predict结果，用作判定请还原。

7.  sklearn参数
